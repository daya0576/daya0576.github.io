---
title: 浅谈 SRE 之监控最佳实践
tags:
---

还记得三年之前，面试 SRE 结束，面试官问我还有什么想问的。我弱弱的问道：如何度量一个系统的稳定性或高可用能力呢？ 面试官笑了一下，说你有没有听过三个九 or 四个九..

这篇文章将简单分享 DevOps 与 SRE 的关系，个人与监控的故事，以及如何使用 SLI/SLO 进行监控的最佳实践～

<!--more-->

# 我心中的 SRE

🤔 首先什么是 SRE(Site Reliability Engineering)？   

- 基础概念：从字面不难理解，即通过软件工程(E)来解决传统的稳定性(SR)或运维问题。   
- 结果目标：生产系统复杂度增长与运维人数不成线性增长，i.e. O(1) 的关系

感兴趣可以阅读[《Site Reliability Engineering》by Google](https://sre.google/books/) or [我的读书笔记](/blog/20180403/impressions-of-google-sre/)

# 名词解释

> SLIs drive SLOs, which inform SLAs

1. **SLIs(Service Level Indicator)**: 
2. **SLOs(Service Level Object)**: 基于 SLAs 的精确目标，例如开篇说到的「四个九」。几个注意点：
    - 生产安全高于一切，虽然需要不断迭代新功能吸引用户，但如果系统不可用会导致损失更多用户，甚至导致公司直接倒闭。。但我们要意识到追求 **100% 的 SLO 是个错误的目标**，因为用户网络设备运营商，以及下游依赖的基础设施的可用率存在平静瓶颈。
    - 从 **用户视角**定义 SLOs，例如 cpu/load 很好理解，但用户不关心你的服务器是几核的，有多少内存，他只在乎暴露的服务是否可用。
3. **SLAs(Service Level Agreement)**: 对外更多的是商业上的约定，破线需要赔付约定的金额，所以通常 SLO 会 SLA 设置的更加严格，保证不破线。
4. **error budget**: 不可用时间 / 整体时间，例如一年 99.99% 的 SLO，就代表你拥有 52min 的不可用时间，用来最大化的发布新的 feature（戴着手铐跳舞）

![](/images/blog/200104_japan_travel/16110702675200.jpg)
p.s. error budget 计算表

# DevOps 和 SRE 的关系

说来惭愧，身为一名 SRE 快三年，一直对 DevOps 这个概念充满困惑。今天终于得到了解答，也趁此机会分享一下～   

**先从传统软件公司的协作模式说起：** 开发编写业务代码，运维负责生产环节的稳定运行，但问题在于：开发不知道他们的代码如何被部署；而代码中业务逻辑对运维来说也完全是一个黑盒。   而这两边的隔阂与信息不对称，很容易将双方的矛盾点不断放大：faster feature VS  production reliability
1. 运维人员（Dev）关注线上的稳定性：如果出现生产可用性故障，需要半夜凌晨起来处理告警，甚至可能自己都被优化
2. 开发同学（Ops）关心如何快速在生产环境交付代码：如果新的特性没有及时发布，他们也会被作为人才向社会输送。。 


在这个背景下 `DevOps` 应运而生，期望通过以下几个方面，加强双边的合作，解决以上紧张的矛盾。而 SRE 则可以简单理解 DevOps 理念或指导思想的**最佳实践**（`class SRE implements DevOps`😄），举两个例子：

1. accept failure as normal: 由人类构建的系统注定是会出错的，所有**对任何事情都要提前准备**，例如建设容灾能力的建设，以及每个变更都必须是可回滚。
2. gradual change: 这里有两层含义
    - 小步快跑：变更代码数越少，出问题时更加轻松定位与回滚（相比于一百万行代码的发布..） 
    - 灰度：每个变更都需要灰度引流，尽可能的提前发现问题，减少影响面。     这里分享一下灰度这个概念的有趣故事：国外又称为金丝雀(`Canary`)，因为很久以前英国的煤矿工人下矿井的时候，为了防止中毒，会待一只对瓦斯十分敏感的金丝雀，如果它有异常这说明瓦斯浓度过高，需要撤离，达到提前预警的作用。  
3. measure: 可量化！不管是每个 SRE 在 toil(纯手工操作) 上消耗的时间，以及下文即将提到的监控，都需要有一个量化的明确指标与数据驱动的决策逻辑。
4. ...


# 我和监控的故事

在蚂蚁，针对所有核心业务，制定一套非常完善的**故障等级定义机制**：例如当苹果代扣成功量当前分钟与上一分钟环比下跌超过 30%，即会触发一条 P1 告警；当持续 10 分钟都未恢复，将开启电话外呼（无情的女声，一生的噩梦😢）：“集团监控告警，故障等级...”

用户的交易支付数据确实存在惊人的周期性，但同时也包含很强的业务属性：例如活动秒杀之冲高回落，外部同步依赖异常，以及小流量业务的频繁抖动等。成功量下跌的告警静态阈值确实有效，但同时也导致了大量误报告警。

![](/images/blog/200104_japan_travel/16110709033461.jpg)


为了解决以上告警问题，我参与了以下三个项目的开发建设：

1. **[业务报警智能降噪](/blog/20190113/anomaly-detection/)**：通过机器学习的基线拟合与异常监测算法，代替人的经验自动判断时序数据是否异常，最终对告警进行降噪抑制。
2. **单笔全风险**：将多个应用的运行态数据，通过 trace 进行关联建模后，编写代码对每一笔流量的「可用性」与「正确性」进行判断，例如：
    1. 可用性：某个业务的支付请求，在入口网关返回给用户未知结果，同时在收单出现预期外的系统异常；巧的是异常对应机器在五分钟前进行了代码发布部署，则立即预警并阻断变更
    2. 正确性：商户发起一笔代扣，由于访问渠道超时，返回未知结果。需要覆盖时效性规则：商户对同笔订单是否在一个小时内发起查询并成功获取终态。否则会造成单边帐，商户侧订单状态与支付宝的状态不一致，影响用户体验甚至客诉。
3. **多维指标联合告警**：结合多个时序监控的指标获取结论。例如 当入口成功量下跌超过 30%，自动检查来源数据是否下跌，或出口依赖的渠道是否异常


# 如何做好监控

> 当前分钟与上一分钟的成功量，环比下跌超过 30%

但三年过去了，回过头来看，告警泛滥背后本质的问题在于：**上面规则其实并不是一个合理的 SLI**。为什么成功量下跌 30%，就代表支付宝内部不可用了呢？

如果从头开始做监控，

1. 定义面向用户或商户的 SLI，
2. 定义具体的 SLO，利用



# 总结

数据驱动，




