# æœºå™¨å­¦ä¹  -  coursera

æœºå™¨å­¦ä¹  -  Andrew Ng(å´æ©è¾¾)
[https://www.coursera.org/learn/machine-learning/home/welcome](https://www.coursera.org/learn/machine-learning/home/welcome)

# ç¬¬ä¸€å‘¨
## æ€»ç»“:
![](IMG_9319.JPG)

## æœºå™¨å­¦ä¹ å®šä¹‰:
> Well-posed Learning Problem: A computer program is said to learnfrom experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. 

`T:` Classifying emails as spam or not spam
`E:` Watching u label emails as spam or not spam
`P:` The number of emails correctly classified as spam

## æœºå™¨å­¦ä¹ ç®—æ³•åˆ†ç±»:
**Supervised Learning**
- `regression`: åœ¨è¿ç»­çš„æ•°æ®ä¸­é¢„æµ‹  
	![](DraggedImage.png)
- `classification`: æœ€å¤§çš„åŒºåˆ«åœ¨äºé¢„æµ‹çš„ç»“æœ, è‚¯å®šä¸ºyes or no, æˆ–è€…ä¸€ä¸ªé›†åˆå†…, e.g. çº¢, é»„, è“, ç»¿  
	![](DraggedImage-1.png)

**Unsupervised learning**
ç»™å®šæ•°æ®, åœ¨ä¸æ ‡æ³¨çš„æƒ…å†µä¸‹, automatically identify structure
- Clustering
- Non-clustering (cocktail party problem: éº¦å…‹é£çš„åˆ†ç¦»(two audio sources))


## Cost Function
linear regression:
- `m` = number of training  examples
- `(x_i, y_i)` â†’ single training example, i: index

**cost function**(squared **error** function) â†’ measure the accuracy
![](DraggedImage-2.png)
a fancier version of an average: ä¸ªäººç†è§£ç”¨å¹³æ–¹å°†ä¸ªåˆ«å·®å¼‚æ”¾å¤§. 
(ä¸ºä»€ä¹ˆè¦é™¤ä»¥2mè€Œä¸æ˜¯m)??(å“‡, ä¸‹ä¸€èŠ‚çš„ç¬”è®°å°±æœ‰è§£é‡Š): The mean is halved(1/2) as a convenience for the computation of the gradient descent
(æ²¡çœ‹æ‡‚, å¸Œæœ›ä¹‹åä¼šæåŠ) â†’ æ±‚å¯¼æ—¶ä¼šå¤šå‡ºä¸€ä¸ª2, åˆšå¥½æŠµæ¶ˆäº†

linear & cost function
![](DraggedImage-3.png)
æ‰€ä»¥ç›®æ ‡å°±æ˜¯æ‰¾åˆ°cost functionçš„æœ€å°å€¼: 
![](DraggedImage-4.png)

å¤ªå¸…äº†.. 
![](DraggedImage-5.png)
![](DraggedImage-6.png)

## Gradient decent
![](DraggedImage-7.png)
âš ï¸æ³¨æ„:
`a := b`: assignment(overwrite aâ€™s value by b)
`a = b`: truth assignment

gradient decent + cost function 
æ±‚å¯¼(partial derivative)ä¹‹å:
![](DraggedImage-8.png)
ä¸ºä»€ä¹ˆÎ¸\_1ä¼šå¤šå‡ºx\_i?? : æ±‚å¯¼æ—¶çš„å‚æ•°.. 
![](DraggedImage-9.png)
åŸå› : 
chain rule: 
[https://zs.symbolab.com/solver/derivative-calculator/%5Cfrac%7Bd%7D%7Bdx%7D%5Cleft(%5Cleft(3x%2B1%5Cright)%5E%7B2%7D%5Cright)](https://zs.symbolab.com/solver/derivative-calculator/%5Cfrac%7Bd%7D%7Bdx%7D%5Cleft(%5Cleft(3x%2B1%5Cright)%5E%7B2%7D%5Cright))
![](DraggedImage-10.png)
![](DraggedImage-11.png)


convex function(bowl shape function) â†’ æ°¸è¿œåªæœ‰ä¸€ä¸ªæœ€ä½ç‚¹

batch gradient descent: 
![](DraggedImage.tiff)

## Linear Algebra Review()
**Matrix**
1. A: [1, 3] â†’ 1 x 2 matrix
2. A: [1, 3] â†’ A\_12 = 3

**Vector**
å®šä¹‰: n x 1 matrix
```matlab
% The ; denotes we are going back to a new row.
A = [1, 2, 3; 4, 5, 6; 7, 8, 9; 10, 11, 12]

% Initialize a vector 
v = [1;2;3] 

% Get the dimension of the matrix A where m = rows and n = columns
[m,n] = size(A)

% You could also store it this way
dim_A = size(A)

% Get the dimension of the vector v 
dim_v = size(v)

% Now let's index into the 2nd row 3rd column of matrix A
A_23 = A(2,3)
```

**ä¸€ä¸ªMatrix çš„åŠ å‡ä¹˜é™¤:**
```matlab
% Initialize matrix A and B 
A = [1, 2, 4; 5, 3, 2]
B = [1, 3, 4; 1, 1, 1]

% Initialize constant s 
s = 2

% See how element-wise addition works
add_AB = A + B 

% See how element-wise subtraction works
sub_AB = A - B

% See how scalar multiplication works
mult_As = A * s

% Divide A by s
div_As = A / s

% What happens if we have a Matrix + scalar?
add_As = A + s
```

**ä¸¤ä¸ªMatrixçš„ç›¸ä¹˜**
å¹¸å¥½ä»¥å‰çº¿æ€§ä»£æ•°å­¦çš„è¿˜ç®—è®¤çœŸ, ä½†ä¸‹è¾¹è¿™ä¸ªç›¸ä¹˜è¿˜æ˜¯æŒºæœ‰æ„æ€çš„, è€Œä¸”ä¼šè®©ä½ çš„codeå˜å¾—simple and efficient:
![](DraggedImage-12.png)
```matlab
% Initialize matrix A 
A = [1, 2, 3; 4, 5, 6;7, 8, 9] 

% Initialize vector v 
v = [1; 1; 1] 

% Multiply A * v
Av = A * v
```
åŒä¸Š:
![](DraggedImage-13.png)

Vectorization
![](DraggedImage-14.png)


**Matrix Multiplication Properties**
1. A x B  !=  B x A (not commutative)
2. (AxB)xC = Ax(BxC) (associate)
3. identity matrix: AxI = IxA  
	![](DraggedImage-15.png)![](DraggedImage-16.png)
4. 

**Matrix inverse**
å¦‚ä½•è®¡ç®—çš„å‘¢?  å¾ˆå°‘æœ‰äººæ‰‹ç®—äº†, ç›´æ¥pinv(A)
![](DraggedImage-17.png)
```matlab
% Transpose A 
A_trans = A' 
```


**Matrix Transpose**
![](DraggedImage-18.png)
```matlab
% Take the inverse of A 
A_inv = inv(A)

% What is A^(-1)*A? 
A_invA = inv(A)*A
```



---- 

# ç¬¬äºŒå‘¨:
MATLAB Online: [https://matlab.mathworks.com/](https://matlab.mathworks.com/)

## æ€»ç»“

## Multiple Features:
1 feature: size â†’ price
n features: size, bedrooms, floors, .. â†’ price
![](IMG_9321.JPG)

**Multivariate linear regression**:
![](DraggedImage-19.png)
![](DraggedImage-20.png)

## Normalization
**Feature Scaling:**
å› ä¸ºä¸åŒçš„featureèŒƒå›´è¿‡å¤§, å¯¼è‡´gradient decentæ•ˆæœå¤ªå·®
feature 1: -1 \<-\> 3
feature 2: -1000 \<-\>  1000

**mean normalization + feature scaling**
![](DraggedImage-21.png)
![](DraggedImage-22.png)


## alphaçš„é€‰æ‹©: 
ä¸èƒ½è‡ªåŠ¨é€‰æ‹©å—?
![](DraggedImage-1.tiff)

## Features and Polynomial Regression
Polynomial: äºŒæ¬¡, ä¸‰æ¬¡, næ¬¡æ–¹ç¨‹, å¤šé¡¹å¼
![](UNADJUSTEDNONRAW_thumb_4ca6.jpg)
æœªæ¥ä¼šä¼ æˆ, å¦‚ä½•è‡ªåŠ¨é€‰æ‹©é€‚åˆçš„function.

## Normal Equation
![](DraggedImage-2.tiff)
å…·ä½“çš„è¯æ˜: [https://blog.csdn.net/Artprog/article/details/51172025](https://blog.csdn.net/Artprog/article/details/51172025)

# ç¬¬ä¸‰å‘¨
## é—®é¢˜å®šä¹‰
è¿™ä¸€å‘¨ä¸»è¦å°±æ˜¯ä¸ºäº†è§£å†³èšç±»é—®é¢˜, å¦‚ä¸‹å›¾:   
![](DraggedImage-3.tiff)

binary classification problem: `{0(negative), 1(positive)}`
multi-classification problem: `{0, 1, 2, 3}`

Linear regressionå¯¹äºèšç±»é—®é¢˜çš„å±€é™æ€§, æ¯”å¦‚ä¸‹å›¾è¿™ä¸ªä¾‹å­, æœ€å³è¾¹çš„ç‚¹å°±å‡ºé”™äº†:
![](DraggedImage-4.tiff)

æ‰€ä»¥æ–°æ¨å‡ºäº†Logistic Function(sigmoid function), ä¸ºäº†è®©hypothesisçš„å€¼æ°¸è¿œåœ¨0å’Œ1ä¹‹é—´:
![](DraggedImage-5.tiff)
```matlab
function g = sigmoid(z)
%SIGMOID Compute sigmoid function
%   g = SIGMOID(z) computes the sigmoid of z.

% You need to return the following variables correctly 
% g = zeros(size(z));

% ====================== YOUR CODE HERE ======================
% Instructions: Compute the sigmoid of each value of z (z can be a matrix,
%               vector or scalar).


g = 1 ./ (1 + exp(-z));


% =============================================================

end
```

Logistic Functionçš„å€¼è¿˜æœ‰å¦å¤–ä¸€ä¸ªå«ä¹‰: ä»£è¡¨è¾“å‡ºç»“æœä¸º1çš„å¯èƒ½æ€§
![](DraggedImage-6.tiff)

Decision Boundary:
![](DraggedImage-7.tiff)
æ‰€ä»¥æ¨å¯¼å‡ºä¸‹é¢ğŸ‘‡ä¸¤ä¸ªç­‰å¼: 
![](DraggedImage-23.png)

Training Set:
![](DraggedImage-24.png)

Cost function:
![](DraggedImage-25.png)


Simplified Cost Function & distance
å°†ä¸Šé¢çš„ä¸¤ä¸ªcost functionåˆå¹¶ä¸ºä¸€ä¸ª:
![](DraggedImage-8.tiff)
æ€»çš„distance å°±ç­‰äº:
![](DraggedImage-9.tiff)
æ±‚å¯¼:
![](DraggedImage-10.tiff)
```matlab
function [J, grad] = costFunction(theta, X, y)
%COSTFUNCTION Compute cost and gradient for logistic regression
%   J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the
%   parameter for logistic regression and the gradient of the cost
%   w.r.t. to the parameters.

% Initialize some useful values
m = length(y); % number of training examples

% ====================== YOUR CODE HERE ======================
% Instructions: Compute the cost of a particular choice of theta.
%               You should set J to the cost.
%               Compute the partial derivatives and set grad to the partial
%               derivatives of the cost w.r.t. each parameter in theta
%
% Note: grad should have the same dimensions as theta
%

% hypothesis:
h = sigmoid(X * theta);
J = 1/m * sum(-y.*log(h) - (1-y).*log(1-h));

grad = 1/m * sum((sigmoid(X * theta) - y) .* X);

% =============================================================

end
```

å¦‚ä½•å¤„ç†multi-dimension classificationçš„é—®é¢˜
![](share.jpg)


**é—®é¢˜:** overfilling 
**è§£å†³åŠæ³•:** 
1. reduce number of features
	1. manually
	2. model selection algorithms
2. Regularization

å¾ˆæœ‰è¶£çš„ä¸€æ®µè¯: 
> â€œWhen I walk around Silicon Valley, I live here in Silicon Valley, there are a lot of engineers that are frankly making a ton of money for their company using machine learning algorithms. And I know weâ€™ve only been, you know studying this stuff for a little while, But if u understand **linear regression, `logistic regression,Â `the advanced optimization algorithms, and regularizaion**, by now, frankly, you probably know quite a lot more machine learning than many, certainly now, but you probably know quite a lot more machine learning right now than frankly, many of the Silicon Valley engineers out there having very successful careers. You know, making tons of money for the companies. Or building products using machine learning algorithms.  â€
> 
# ç¬¬å››å‘¨
## Non-linear Hypothesis:
100 features instead of two. 
50\*50 pixel images â†’ n=2500(pixels)
![](DraggedImage-26.png)

Quadratic features:  
2500: (x\_1, x\_1), (x\_1, x\_2), (x\_1, x\_2), ....
2499: (x\_2, x\_2), (x\_2, x\_3), (x\_2, x\_3), ....
...
answer = 2500 + 2499 + ... + 1 = 3 million

## Neurons and the Brain
Neural Networks â†’ mimic the brain
ä½†æ˜¯è€—è´¹çš„èµ„æºå¤ªå¤šäº†, ç›´åˆ°æœ€è¿‘æ‰æœ‰æ¯”è¾ƒå¥½çš„è®¡ç®—èƒ½åŠ›å»æ‰§è¡Œ. 

Neuron: ç¥ç»å…ƒ

(brainçš„æ¨¡æ‹Ÿå›¾)
![](DraggedImage-27.png)

 bias unit: x\_0

è®¡ç®—ç¬¬äºŒå±‚:
![](DraggedImage-11.tiff)
è®¡ç®—ç¬¬ä¸‰å±‚: 
![](DraggedImage-28.png)


# ä¹±ä¸ƒå…«ç³Ÿ:
ç¼–ç¨‹è¯­è¨€: Matlab or Octave
åŸå› : 
> your coding time is the most valuable resource. 

![](DraggedImage-29.png)

matrix, vector or scalar: çŸ©é˜µ, å‘é‡, æ•°é‡

theta: Î¸
alpha: Î±
lambda: Î»

constant e: exp(1)

slope: æ–œç‡
converge: æ”¶æ•›
derivative: å¯¼æ•°

