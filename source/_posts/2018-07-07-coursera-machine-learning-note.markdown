---
title: æœºå™¨å­¦ä¹ (Coursera - Andrew NG)ä¸ªäººç¬”è®°
tags:
  - study
  - coursera
comments: true
date: 2018-07-07 10:17:32
---


ä¸€ç›´è§‰å¾—Courseraæ˜¯ä¸ªå¾ˆå¥½çš„å­¦ä¹ å¹³å°, å› ä¸ºè¯¾ç¨‹ä¸­éƒ½ä¼šç©¿æ’ç€ä¸°å¯Œçš„**è¯¾åä½œä¸š**å’Œ**å¤§ä½œä¸š**(çœŸæ­£æŒæ¡çŸ¥è¯†è¿˜æ˜¯éœ€è¦ä¸»åŠ¨æ€è€ƒğŸ¤”å’Œå®è·µ). 
è€Œä¸”å¾ˆå·§, å››å¹´å¤šå‰åˆšå¼€å§‹å†™åšå®¢æ—¶, è®°å½•çš„æ˜¯å­¦ä¹ ã€ŠAn Introduction to Interactive Programming in Pythonã€‹çš„è¿‡ç¨‹. å½“æ—¶æ¯ä¸ªäººçš„å¤§ä½œä¸šéƒ½æ˜¯éšæœºäº”ä¸ªåŒå­¦æ‰¹æ”¹çš„, [è¿™ç¯‡åšå®¢](/blog/20140429/cousera-comments/)è®°å½•äº†æ¥è‡ªä¸–ç•Œå„åœ°çš„äººå¯¹æˆ‘ä½œä¸šçš„è¯„è®ºğŸ˜„).    
å››å¹´åé‡æ–°å‡ºå‘, å­¦ä¹ Andrew Ngçš„[ã€ŠMachine Learningã€‹](https://www.coursera.org/learn/machine-learning/home/welcome)), åˆ†äº«ä¸€ä¸‹è‡ªå·±çš„ç¬”è®°å’Œå°å°æ„Ÿæ‚Ÿ.

<!--more-->

# ç¬¬ä¸€å‘¨
## æ€»ç»“:
![](/2018-07-07-coursera-machine-learning-note/week1_summary.png)

## æœºå™¨å­¦ä¹ å®šä¹‰:
> Well-posed Learning Problem: A computer program is said to learnfrom experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.    
> `T:` Classifying emails as spam or not spam.  
> `E:` Watching u label emails as spam or not spam.  
> `P:` The number of emails correctly classified as spam.  


## æœºå™¨å­¦ä¹ ç®—æ³•åˆ†ç±»:
### Supervised Learning
- `regression`: åœ¨è¿ç»­çš„æ•°æ®ä¸­é¢„æµ‹       
	<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage.png">      
- `classification`: æœ€å¤§çš„åŒºåˆ«åœ¨äºé¢„æµ‹çš„ç»“æœ, è‚¯å®šä¸ºyes or no, æˆ–è€…ä¸€ä¸ªé›†åˆå†…, e.g. çº¢, é»„, è“, ç»¿  
	<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-1.png">     

### Unsupervised learning
ç»™å®šæ•°æ®, åœ¨ä¸æ ‡æ³¨çš„æƒ…å†µä¸‹, automatically identify structure
- Clustering
- Non-clustering (cocktail party problem: éº¦å…‹é£çš„åˆ†ç¦»(two audio sources))


## Cost Function
linear regression:
- `m` = number of training  examples
- `(x_i, y_i)` â†’ single training example, i: index

**cost function**(squared **error** function) â†’ measure the accuracy
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-2.png).">        
a fancier version of an average: ä¸ªäººç†è§£ç”¨å¹³æ–¹å°†ä¸ªåˆ«å·®å¼‚æ”¾å¤§.    
(ä¸ºä»€ä¹ˆè¦é™¤ä»¥2mè€Œä¸æ˜¯m)??(å“‡, ä¸‹ä¸€èŠ‚çš„ç¬”è®°å°±æœ‰è§£é‡Š): The mean is halved(1/2) as a convenience for the computation of the gradient descent.    
(æ²¡çœ‹æ‡‚, å¸Œæœ›ä¹‹åä¼šæåŠ) â†’ æ±‚å¯¼æ—¶ä¼šå¤šå‡ºä¸€ä¸ª2, åˆšå¥½æŠµæ¶ˆäº†.   

linear & cost function
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-3.png">        
æ‰€ä»¥ç›®æ ‡å°±æ˜¯æ‰¾åˆ°cost functionçš„æœ€å°å€¼: 
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-4.png">     

å¤ªå¸…äº†..    
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-5.png">     
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-6.png">     

## Gradient decent
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-7.png">     
âš ï¸æ³¨æ„:
`a := b`: assignment(overwrite aâ€™s value by b)
`a = b`: truth assignment

gradient decent + cost function 
æ±‚å¯¼(partial derivative)ä¹‹å:
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-8.png">     
ä¸ºä»€ä¹ˆÎ¸\_1ä¼šå¤šå‡ºx\_i?? : æ±‚å¯¼æ—¶çš„å‚æ•°..    
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-9.png">     
åŸå› :    
chain rule:    
[https://zs.symbolab.com/solver/derivative-calculator/%5Cfrac%7Bd%7D%7Bdx%7D%5Cleft(%5Cleft(3x%2B1%5Cright)%5E%7B2%7D%5Cright)](https://zs.symbolab.com/solver/derivative-calculator/%5Cfrac%7Bd%7D%7Bdx%7D%5Cleft(%5Cleft(3x%2B1%5Cright)%5E%7B2%7D%5Cright))
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-10.png">     
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-11.png">     

convex function(bowl shape function) â†’ æ°¸è¿œåªæœ‰ä¸€ä¸ªæœ€ä½ç‚¹

batch gradient descent: 
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage.tiff">     

## Linear Algebra Review
**Matrix**
1. A: [1, 3] â†’ 1 x 2 matrix
2. A: [1, 3] â†’ A\_12 = 3

**Vector**
å®šä¹‰: n x 1 matrix
```matlab
% The ; denotes we are going back to a new row.
A = [1, 2, 3; 4, 5, 6; 7, 8, 9; 10, 11, 12]

% Initialize a vector 
v = [1;2;3] 

% Get the dimension of the matrix A where m = rows and n = columns
[m,n] = size(A)

% You could also store it this way
dim_A = size(A)

% Get the dimension of the vector v 
dim_v = size(v)

% Now let's index into the 2nd row 3rd column of matrix A
A_23 = A(2,3)
```

**ä¸€ä¸ªMatrix çš„åŠ å‡ä¹˜é™¤:**
```matlab
% Initialize matrix A and B 
A = [1, 2, 4; 5, 3, 2]
B = [1, 3, 4; 1, 1, 1]

% Initialize constant s 
s = 2

% See how element-wise addition works
add_AB = A + B 

% See how element-wise subtraction works
sub_AB = A - B

% See how scalar multiplication works
mult_As = A * s

% Divide A by s
div_As = A / s

% What happens if we have a Matrix + scalar?
add_As = A + s
```

### ä¸¤ä¸ªMatrixçš„ç›¸ä¹˜
å¹¸å¥½ä»¥å‰çº¿æ€§ä»£æ•°å­¦çš„è¿˜ç®—è®¤çœŸ, ä½†ä¸‹è¾¹è¿™ä¸ªç›¸ä¹˜è¿˜æ˜¯æŒºæœ‰æ„æ€çš„, è€Œä¸”ä¼šè®©ä½ çš„codeå˜å¾—simple and efficient:
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-12.png">     
```matlab
% Initialize matrix A 
A = [1, 2, 3; 4, 5, 6;7, 8, 9] 

% Initialize vector v 
v = [1; 1; 1] 

% Multiply A * v
Av = A * v
```
åŒä¸Š:    
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-13.png">     

Vectorization
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-14.png">     


### Matrix Multiplication Properties    
1. A x B  !=  B x A (not commutative)
2. (AxB)xC = Ax(BxC) (associate)
3. identity matrix: AxI = IxA  
	<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-15.png)<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-16.png">     


### Matrix inverse
å¦‚ä½•è®¡ç®—çš„å‘¢?  å¾ˆå°‘æœ‰äººæ‰‹ç®—äº†, ç›´æ¥pinv(A)
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-17.png">     
```matlab
% Transpose A 
A_trans = A' 
```


### Matrix Transpose
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-18.png">     
```matlab
% Take the inverse of A 
A_inv = inv(A)

% What is A^(-1)*A? 
A_invA = inv(A)*A
```


# ç¬¬äºŒå‘¨:
MATLAB Online: [https://matlab.mathworks.com/](https://matlab.mathworks.com/)   
å¤ªTMéš¾ç”¨äº†...

## Multiple Features:
1 feature: size â†’ price
n features: size, bedrooms, floors, .. â†’ price
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\IMG_9321.JPG">     

**Multivariate linear regression**:
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-19.png">     
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-20.png">     

## Normalization
**Feature Scaling:**
å› ä¸ºä¸åŒçš„featureèŒƒå›´è¿‡å¤§, å¯¼è‡´gradient decentæ•ˆæœå¤ªå·®
feature 1: -1 \<-\> 3
feature 2: -1000 \<-\>  1000

**mean normalization + feature scaling**
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-21.png">     
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-22.png">     


## alphaçš„é€‰æ‹©: 
ä¸èƒ½è‡ªåŠ¨é€‰æ‹©å—?
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-1.tiff">     

## Features and Polynomial Regression
Polynomial: äºŒæ¬¡, ä¸‰æ¬¡, næ¬¡æ–¹ç¨‹, å¤šé¡¹å¼
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\UNADJUSTEDNONRAW_thumb_4ca6.jpg">     
æœªæ¥ä¼šä¼ æˆ, å¦‚ä½•è‡ªåŠ¨é€‰æ‹©é€‚åˆçš„function.

## Normal Equation
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-2.tiff">     
å…·ä½“çš„è¯æ˜: [https://blog.csdn.net/Artprog/article/details/51172025](https://blog.csdn.net/Artprog/article/details/51172025)


# ç¬¬ä¸‰å‘¨
è¿™ä¸€å‘¨ä¸»è¦å°±æ˜¯ä¸ºäº†è§£å†³èšç±»é—®é¢˜, å¦‚ä¸‹å›¾:   
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml/classification.png">     

## Classigicationçš„åˆ†ç±»
1. åªèšç±»ä¸º1æˆ–0(binary classification problem): `{0(negative), 1(positive)}`   
2. èšç±»ä¸ºä¸€ä¸ªé›†åˆ(multi-classification problem): `{0, 1, 2, 3}`

## Linear regressionå¯¹äºèšç±»é—®é¢˜çš„å±€é™æ€§
æ¯”å¦‚ä¸‹å›¾è¿™ä¸ªä¾‹å­, æœ€å³è¾¹çš„ç‚¹å°±å‡ºé”™äº†:   
<img style="max-height:200px" class="lazy" data-original="\images\blog\180707_cousera_ml\limitation.png">     

## Hypothesis Representation
### Logistic Function
æ‰€ä»¥æ–°æ¨å‡ºäº†Logistic Function(sigmoid function), ç›®çš„æ˜¯ä¸ºäº†è®©hypothesisçš„å€¼æ°¸è¿œåœ¨0å’Œ1ä¹‹é—´:    
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml/sigmoid.png">     
**å…·ä½“å®ç°:**   
```matlab
function g = sigmoid(z)
%SIGMOID Compute sigmoid function
%   g = SIGMOID(z) computes the sigmoid of z.

% You need to return the following variables correctly 
% g = zeros(size(z));

% ====================== YOUR CODE HERE ======================
% Instructions: Compute the sigmoid of each value of z (z can be a matrix,
%               vector or scalar).


g = 1 ./ (1 + exp(-z));


% =============================================================

end
```

Logistic Functionçš„å€¼å…¶å®è¿˜æœ‰å¦å¤–ä¸€ä¸ªå«ä¹‰: ä»£è¡¨è¾“å‡ºç»“æœä¸º1çš„å¯èƒ½æ€§:   
<img style="max-height:200px" class="lazy" data-original="\images\blog\180707_cousera_ml/logistic_function_poss.png">     

### Decision Boundary:    
<img style="max-height:200px" class="lazy" data-original="\images\blog\180707_cousera_ml/Boundary.png">     
æ‰€ä»¥æ¨å¯¼å‡ºä¸‹é¢ğŸ‘‡ä¸¤ä¸ªç­‰å¼:    
<img style="max-height:200px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-23.png">     

### Training Set:
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-24.png">     

### Cost function:
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-25.png">     

## Simplified Cost Function & distance
å°†ä¸Šé¢çš„ä¸¤ä¸ªcost functionåˆå¹¶ä¸ºä¸€ä¸ª, å’Œç¬¬äºŒå‘¨çš„regressionç±»ä¼¼, æ€»çš„distanceå°±ç­‰äº:
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml/distance.png">     

distanceæ±‚å¯¼(è¯´å®è¯æ²¡çœ‹æ‡‚, è§†é¢‘ä¸­åªç»™äº†ç»“æœ, å¯èƒ½æ¨åˆ°æ¯”è¾ƒå¤æ‚ä¸€äº›):
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml/distance_gradient.png">     
```matlab
function [J, grad] = costFunction(theta, X, y)
%COSTFUNCTION Compute cost and gradient for logistic regression
%   J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the
%   parameter for logistic regression and the gradient of the cost
%   w.r.t. to the parameters.

% Initialize some useful values
m = length(y); % number of training examples

% ====================== YOUR CODE HERE ======================
% Instructions: Compute the cost of a particular choice of theta.
%               You should set J to the cost.
%               Compute the partial derivatives and set grad to the partial
%               derivatives of the cost w.r.t. each parameter in theta
%
% Note: grad should have the same dimensions as theta
%

% hypothesis:
h = sigmoid(X * theta);
J = 1/m * sum(-y.*log(h) - (1-y).*log(1-h));

grad = 1/m * sum((sigmoid(X * theta) - y) .* X);

% =============================================================

end
```

## å¦‚ä½•å¤„ç†multi-dimension classificationçš„é—®é¢˜
æ„æ€å°±æ˜¯è¿›è¡Œå¤šæ¬¡çš„äºŒæ¬¡èšç±»:    
<img style="max-height:300px" class="lazy" data-original="/images/blog/180707_cousera_ml/share.jpg">     

## Overfitting, æ„æ€å°±æ˜¯è¯´è¿‡åº¦çš„æ‹Ÿåˆ(æ²¡çœ‹æ‡‚, æ”¾å¼ƒäº†)

## å¾ˆæœ‰è¶£çš„ä¸€æ®µè¯: 
> â€œWhen I walk around Silicon Valley, I live here in Silicon Valley, there are a lot of engineers that are frankly making a ton of money for their company using machine learning algorithms. And I know weâ€™ve only been, you know studying this stuff for a little while, But if u understand **linear regression, `logistic regression, `the advanced optimization algorithms, and regularizaion**, by now, frankly, you probably know quite a lot more machine learning than many, certainly now, but you probably know quite a lot more machine learning right now than frankly, many of the Silicon Valley engineers out there having very successful careers. You know, making tons of money for the companies. Or building products using machine learning algorithms.  â€



# ç¬¬å››å‘¨
## Non-linear Hypothesis:
100 features instead of two. 
50\*50 pixel images â†’ n=2500(pixels)
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-26.png">     

Quadratic features:  
2500: (x\_1, x\_1), (x\_1, x\_2), (x\_1, x\_2), ....
2499: (x\_2, x\_2), (x\_2, x\_3), (x\_2, x\_3), ....
...
answer = 2500 + 2499 + ... + 1 = 3 million

## Neurons and the Brain
Neural Networks â†’ mimic the brain
ä½†æ˜¯è€—è´¹çš„èµ„æºå¤ªå¤šäº†, ç›´åˆ°æœ€è¿‘æ‰æœ‰æ¯”è¾ƒå¥½çš„è®¡ç®—èƒ½åŠ›å»æ‰§è¡Œ. 

Neuron: ç¥ç»å…ƒ

(brainçš„æ¨¡æ‹Ÿå›¾)
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-27.png">     

 bias unit: x\_0

è®¡ç®—ç¬¬äºŒå±‚:
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-11.tiff">     
è®¡ç®—ç¬¬ä¸‰å±‚: 
<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-28.png">     


# ä¹±ä¸ƒå…«ç³Ÿ:
ç¼–ç¨‹è¯­è¨€: Matlab or Octave
åŸå› : 
> your coding time is the most valuable resource. 

<img style="max-height:300px" class="lazy" data-original="\images\blog\180707_cousera_ml\DraggedImage-29.png">     

matrix, vector or scalar: çŸ©é˜µ, å‘é‡, æ•°é‡

theta: Î¸
alpha: Î±
lambda: Î»

constant e: exp(1)

slope: æ–œç‡
converge: æ”¶æ•›
derivative: å¯¼æ•°






