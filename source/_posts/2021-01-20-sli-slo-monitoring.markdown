---
title: 浅谈 SRE 之监控
date: 2021-01-20 23:43:39
tags:
  - sre
---


还记得三年之前，面试 SRE 结束，面试官问我还有什么想问的。我弱弱的问道：如何度量一个系统的稳定性或高可用能力呢？ 面试官笑了一下，说你有没有听过“3 个 9”或“4 个 9”吗..

这篇文章将简单分享 DevOps 与 SRE 的关系，结合个人与监控的故事，谈谈如何使用 SLI/SLO 进行监控的最佳实践～

<!--more-->

# 我心中的 SRE

🤔 首先什么是 SRE(Site Reliability Engineering)？   

- **基础概念：**从字面不难理解，即通过软件工程(Engineering)来解决大型系统复杂的稳定性(Site Reliability)或运维问题。   
- **结果目标：**生产系统复杂度增长与运维人数不成线性增长

感兴趣可以阅读[《Site Reliability Engineering》by Google](https://sre.google/books/) or 直接阅读 [我的读书笔记](/blog/20180403/impressions-of-google-sre/)

# 核心名词解释

> SLIs drive SLOs, which inform SLAs

1. **SLIs(Service Level Indicator)**: **监控指标**明确判断当前服务的每笔流量是否可用
    - 注意尽量从 **用户视角** 定义 SLIs，例如 cpu/load 虽然很好理解，但用户不关心你的服务器是几核的，有多少内存，他只在乎暴露的支付服务是否可用。
2. **SLOs(Service Level Object)**: 基于 SLAs 的精确目标，例如 99.95% 的用户在未来一周都能在 200ms 内返回预期结果。几个注意点：
    - 虽然生产安全高于一切，但注意追求 **100% 的 SLO 是个错误的目标**，因为用户网络设备运营商，以及下游依赖的基础设施的可用率存在平静瓶颈。
3. **SLAs(Service Level Agreement)**: 对外更多的是**商业上的书面约定**，破线需要赔付约定的金额，所以通常 SLO 会 SLA 设置的更加严格，保证提前预警。
4. **error budget**: 不可用时间 / 整体时间，例如一年 99.99% 的 SLO，就代表你拥有 52min 的不可用时间，用来最大化的发布新的 feature（戴着手铐跳舞）

![](/images/blog/200104_japan_travel/16110702675200.jpg)


# DevOps 和 SRE 的关系

说来惭愧，身为一名 SRE 快三年，一直对 DevOps 这个概念充满困惑。今天终于得到了解答，也趁此机会分享一下～   

**先从传统软件公司的协作模式说起：** 开发编写业务代码，运维负责生产环节的稳定运行，但问题在于：开发不知道他们的代码如何被部署；而代码中业务逻辑对运维来说也完全是一个黑盒。   而这两边的隔阂与信息不对称，很容易将双方的矛盾点不断放大：faster feature VS  production reliability
1. 运维人员（Dev）关注线上的稳定性：如果出现生产可用性故障，需要半夜凌晨起来处理告警，甚至可能自己都被优化
2. 研发同学（Ops）关心如何快速在生产环境交付代码：如果新的特性没有及时发布，他们也会被作为人才向社会输送。。（） 


在这个背景下 `DevOps` 应运而生，期望通过以下几个方面，解决以上紧张的矛盾。而 SRE 则可以简单理解 DevOps 理念或指导思想的**最佳实践**（`class SRE implements DevOps`😄），举两个例子：

1. accept failure as normal: 由人类构建的系统注定是会出错的，所有**对任何事情都要提前准备**，例如建设容灾能力的建设，以及每个变更都必须是可回滚。
2. gradual change: 这里有两层含义
    - 小步快跑：变更代码数越少，出问题时更加轻松定位与回滚（相比于一百万行代码的发布..） 
    - 灰度：每个变更都需要灰度引流，尽可能的提前发现问题，减少影响面。     这里分享一下灰度这个概念的有趣故事：国外又称为金丝雀(`Canary`)，因为很久以前英国的煤矿工人下矿井的时候，为了防止中毒，会待一只对瓦斯十分敏感的金丝雀，如果它有异常这说明瓦斯浓度过高，需要撤离，达到提前预警的作用。  
3. measure: 可量化！不管是每个 SRE 在 toil(纯手工操作) 上消耗的时间，以及下文即将提到的监控，都需要有一个量化的明确指标与数据驱动的决策逻辑。
4. ...


# 我和监控的故事

在蚂蚁，针对所有核心业务，都制定一套非常完善的**故障等级定义机制**：例如当苹果代扣成功量当前分钟与上一分钟环比下跌超过 20%，即会触发一条 P1 告警；当持续 10 分钟都未恢复，将开启电话外呼（无情的女声，一生的噩梦😢）：“集团监控告警，故障等级...”

用户的交易支付数据确实存在惊人的周期性，但同时也包含很强的业务属性：例如活动秒杀之冲高回落，外部同步依赖异常，以及小流量业务的频繁抖动等。成功量下跌的静态阈值确实有效，但同时也导致了大量误报告警。

![](/images/blog/200104_japan_travel/16110709033461.jpg)


为了解决以上告警问题，我参与了以下三个项目的开发建设：

1. **[业务报警智能降噪](/blog/20190113/anomaly-detection/)**：通过机器学习的基线拟合与异常监测算法，代替人的经验自动判断时序数据是否异常，最终对告警进行降噪抑制。
2. **单笔全风险**：将多个应用的业务日志，通过 trace 进行关联后建模，通过代码编写对每一笔流量的「可用性」与「正确性」进行判断，例如：
    1. 可用性：某个业务的支付请求，在入口网关返回给用户未知结果，同时在收单出现预期外的系统异常；巧的是异常对应机器在五分钟前进行了代码 beta 发布部署，则立即预警并阻断并回滚变更
    2. 正确性：商户发起一笔代扣，由于访问渠道超时，返回未知结果(U)。需要覆盖时效性规则：商户对同笔订单是否在一个小时内发起查询并成功获取终态。否则会造成单边帐：商户侧订单状态与支付宝的状态不一致(U<->S/F)，影响用户体验甚至客诉。
3. **多维指标联合告警**：结合多个时序监控的指标获取结论。例如入口成功量下跌超过 30%，自动检查来源数据是否下跌，或出口依赖的渠道是否异常等，断言内部服务是否正常。


# 如何做好监控

> 当前分钟与上一分钟的成功量，环比下跌超过 30%

但三年过去了，回过头来看，告警泛滥背后本质的问题在于：**上面规则其实并不是一个合理的 SLI**。成功量下跌 30% 与支付服务不可用不是充分条件。而在这个不合理的 SLA 之上，我们利用人工智能算法和专家经验规则，来各种 workaround 降噪，自然无法根治监控这一顽疾。

如果从头开始做监控，应该简单 follow 以下两个步骤：

1. 重新定义清楚面向用户或商户的 SLI：可以明确判断每笔流量是否异常（或者当前分钟自身服务是否可用）。
2. 定义具体的 SLO，利用 error budget 动态控制变更频率。甚至当一个周期内，某个业务服务的 error budget 耗尽，SRE 有权中止和拒绝任何线上变更。

---

多说一句，其实想做好任何事情都一样：最最重要的是先对目标量化，再通过数据驱动的科学方法，不断控制提升。写给自己的忠告与共勉。 






