---
title: 浅谈 SRE & 业务监控
date: 2021-01-20 23:43:39
tags:
  - sre
---

还记得两年之前，青涩的我第一次面试 SRE 职位快结束，面试官问还有什么问题吗？我弱弱的问道：如何去度量支付宝自身的稳定性或高可用能力呢？ 面试官笑了一下，说你有没有听过“3 个 9”或“4 个 9”呢..  

这篇文章将分享两年后，我对 SRE 与 DevOps 的理解；同时结合个人与监控的故事，谈谈如何定义更加饱满的 SLI，达到预期的 SLO，让 SRE 吃的更香睡的更好 XD

<!--more-->


# 我心中的 SRE

**🤔 首先什么是 SRE(Site Reliability Engineering)？**   

- **过程：**从字面不难理解，即通过软件工程(Engineering)来解决日益复杂的线上稳定性问题(Site Reliability)。
- **结果：**最终达到「线上复杂度」增长与「运维人数」的扩张不成线性增长的目标。但我理解可以是 log(N) 的复杂度，也就是说当产品用户量从1万用户增长至1亿的用户后，业务增长一万倍，运维人员只需要增长十倍。

感兴趣可以阅读官方介绍 [《Site Reliability Engineering》by Google](https://sre.google/books/) or 直接阅读 [我的读书笔记](/blog/20180403/impressions-of-google-sre/)


# DevOps 和 SRE 的关系

身为一名 SRE 快三年，一直对 DevOps 这个概念充满困惑。今天终于从一个视频得到了解答，也趁此机会分享一下～   

**先从传统的协作模式说起：** 开发编写业务代码，运维负责生产环节的稳定运行，但问题在于：开发不知道他们的代码如何被部署；而代码中业务逻辑对运维来说也完全是一个黑盒。 而这两边的信息不对称，很容易将双方的矛盾点不断放大（faster feature VS production reliability）： 

1. 运维人员（Dev）关注线上的稳定性：如果出现生产可用性故障，需要半夜凌晨起来处理告警（而且大概率研发代码的缺陷）
2. 研发同学（Ops）则希望快速交付代码，新的特性没有及时发布，他们也会有麻烦。

在这个背景下 `DevOps` 应运而生，期望通过「研发」与「运维」更加紧密的合作，来解决以上紧张的矛盾。而 SRE 则可以简单理解 DevOps 理念或指导思想的**最佳实践**（`class SRE implements DevOps`😄）。

<iframe width="560" height="315" src="https://www.youtube.com/embed/uTEL8Ff1Zvk" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

# SLI/SLO 监控理论

## 名词解释

1. **SLIs(Service Level Indicator)**: 明确判断当前服务是否可用的监控指标。注意尽量从 **用户视角** 定义 SLIs，例如 cpu/load 虽然很好理解，但用户不关心你的服务器是几核的，有多少内存，他们只在乎暴露的支付或转账服务是否可用。
2. **SLOs(Service Level Object)**: 基于 SLAs 的可量化目标，例如 99.95% 的用户请求在未来一周，都能在 200ms 内返回预期结果。 虽然公司内部常提生产安全高于一切，但注意过分追求 **100% 的 SLO 是个错误的目标**，因为这很难，同时用户网络设备运营商，以及机房依赖的基础设施的可用率本身存在瓶颈。
3. **SLAs(Service Level Agreement)**: 更多的是**商业上的书面约定**，破线需要赔付约定的金额，所以通常 SLO 会 SLA 设置的更加严格，保证提前预警。
4. **error budget**: 不可用时间 / 整体时间，例如一年 99.99% 的 SLO，就代表你拥有 52min 的不可用时间，用来最大化的发布新的 feature（戴着手铐跳舞）

p.s. 下图代表不同的 SLO 对应的 error budget：

![](/images/blog/200104_japan_travel/16110702675200.jpg)


## 关联关系

> SLIs drive SLOs, which inform SLAs

定义清晰的 SLIs，计算 SLOs(开头面试官说的“3 个 9”或“4 个 9”)，动态控制线上风险，例如变更频率等。


# 我和监控的故事

在蚂蚁，针对所有的线上核心业务，都制定一套非常完善的**故障等级定义机制(P1-P4)**：例如当「苹果代扣业务」成功量当前分钟与上一分钟环比下跌超过 20%，即触发一条 P2 等级的告警；当持续 5 分钟都未恢复，将开启电话外呼。

用户的交易支付数据确实存在惊人的周期性，但同时也包含**很强的业务属性**：例如活动秒杀之冲高回落，下游外部同步依赖异常，以及小流量业务的频繁抖动等等。

成功量下跌的静态阈值固然有效，但同时也导致了大量误报（牺牲 precision 来保证 recall）。轻则浪费了大量的人力成本，重则导致告警麻木（狼来了），真正的异常被淹没在噪音中。

![](/images/blog/200104_japan_travel/16110709033461.jpg)


## 1. 噪音的本质

> 当前分钟与上一分钟的成功量，环比下跌超过 30%

但三年过去了，回过头来看告警泛滥背后本质的问题在于：**故障等级定义规则被大家误作为 SLI**，！！但是成功量下跌 30% 与支付服务不可用不是充分条件（其中包含一个隐含条件：因支付宝自身的技术问题导致）。

而在这个不合理的 SLI 之上，我们利用人工智能算法和专家经验规则，对成功量下跌触发的告警做各种 workaround 降噪，自然无法高效根治告警误报。


## 2. 理想中的监控

如果从头开始做监控，可以简单 follow 以下两个步骤：

1. **重新定义清楚面向用户或商户的 SLI**：明确判断每笔流量，或当前分钟的支付指标是否因为技术原因导致不可用。
2. **定义 SLO 后，实时计算 error budget 动态控制变更频率**。甚至当一个周期内，某个业务服务的 error budget 耗尽，SRE 有权中止和拒绝任何线上变更。


## 3. 回到现实

但现实世界中因为资源等因素的限制，我们很难一步配置理想（非黑即白）的 SLI，所以我们需要像计算机存储体系一样，对「可观测指标」进行分类，利用各自的优势定义更加饱满的 SLI 体系。

但在此之前我们先梳理当前可用的「可观测指标」：分为三个维度（参考《[Metrics, tracing, and logging](https://peter.bourgon.org/blog/2017/02/21/metrics-tracing-and-logging.html)》），从上往下代表对存储容量的需求，同时对异常检测规则的精度要求也更高：

1. **Logging:** 应用单条日志（**一次 DB 变更，甚至消息等**），代表一条原始数据
2. **Tracing:** 通过 traceid **或外部单号串联用户行为**，例如用户一笔订单支付的上下文（咨询/支付/通知）。
3. **Metrics:** 累加聚合后的指标型数据，例如每分钟的总量，成功率，平均耗时等。

![](/images/blog/200104_japan_travel/16150063878188.jpg)

## 4. 更加饱满的 SLIs

### 1) Metric 
记得读 Google SRE 书中提到，SLI 要尽可能保持简单，但在复杂的业务场景下，通过**单指标（例如成功量下跌）**来判断系统是否可用，是很难行得通的。所以首先针对最上层 Metrics 做了两件事情：

1. **[告警智能降噪](/blog/20190113/anomaly-detection/)**：通过机器学习的基线拟合与异常监测算法，代替人的经验判断时序数据是否异常，最终对告警进行降噪抑制。
3. **多维指标联合告警**：基于多个时序监控的 Metric 指标综合判断。例如入口成功量下跌超过 30%，自动检查来源监控数据是否下跌，或出口依赖的渠道是否异常等，最终断言内部服务的可用性。

⚠️ 划重点，两个注意点： 1）跳出被动降噪的思维定式，当算法或多指标检测异常时，主动触发预警。 2）告警分级(区分「技术应急」与「业务应急」): 当异常是由于来源或者下游渠道等外部因素导致的，需要将告警自动分发至技术支持进行「业务应急」；判断为内部异常时，电话外呼拉起研发/SRE 进行「技术应急」。

### 2) Tracing 

Metrics 通过原始日志的分钟聚合节约存储空间，但同时牺牲了丰富的原始信息。上文提到多指标联合告警，手动关联多个 metric 指标。理论上可以通过 tracing 进行流式计算，自动关联全链路的每笔请求后，进行单笔或趋势的预警。举几个例子：

1. **可用性：**某个业务的支付请求，在入口网关返回给用户未知结果，在收单出现预期外的系统异常，同时系统异常对应的收单机器，刚好在三分钟前进行了代码 beta 发布部署，则立即预警并回滚变更
2. **正确性：**商户发起一笔代扣，由于支付宝访问渠道超时，出口返回未知结果(U)，需要覆盖：
    - 时效性规则：商户对同笔订单是否在一个小时内发起查询并成功获取终态，否则会造成单边帐：商户侧订单状态与支付宝的状态不一致，影响用户体验甚至客诉。
    - 一致性规则：同步返回给商户的结果状态不能从未知变为成功或失败(U->S/F)，否则妥妥的资损。
    
### 3) Logging

针对每一笔原始的数据，同样可以布防零容忍规则，历史 NPE，证书过期等等。


# THE END

通过发挥 Metric, Tracing, Logging 各自的优势，定义出更加“饱满”的精准 SLI 减少告警误报漏，最终达到预设的 SLO，让 SRE 睡的更香～

![](/images/blog/200104_japan_travel/16150178088094.jpg)

本文通过过去两年多的线上监控经历，总结出个人视角关于 SLI 的最佳实践。

但终究只是在公司技术体系视角内的井底之蛙，如果你在业内或社区有更加优雅的解决方案，欢迎随时联系我进行交流～～


# 参考

1. https://wu-sheng.github.io/me/articles/metrics-tracing-and-logging.html


--- 

p.s. 分享一下灰度这个概念的有趣小故事（国外又称为金丝雀`Canary`）：很久以前英国的煤矿工人下矿井的时候，为了防止中毒，会待一只对瓦斯十分敏感的金丝雀，如果它有异常这说明瓦斯浓度过高，需要撤离，达到提前预警的作用 XD  

